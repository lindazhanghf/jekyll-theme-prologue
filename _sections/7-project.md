---
title: Project
icon: fa-sign-language
order: 7
---
### [Video Demo](http://lindazhanghf.github.io/ms-video)


<iframe width="560" height="315" src="http://lindazhanghf.github.io/ms-video" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br/>


### Documents

For detailed documentations, please refer to: <a href="https://drive.google.com/open?id=1I5cARrjF9TYZXxFUK9tdSBzoi-KT6lu5" target="_blank"><b>Project Documentation</b></a> and <a href="https://drive.google.com/file/d/1nhaJ_J6LrW3Z77LXhWHf5HB06dS9iRtK/view?usp=sharing" target="_blank"><b>Defence Presentation</b></a>. The initial idea of this project is presented in: <a href="https://drive.google.com/file/d/1Bx-c34DvuB2ukwHQm0PzIPzXEenF4HW1/view?usp=sharing" target="_blank"><b>Project Proposal</b></a>.

### Evaluation
No formal evaluation was possible, due to the Covid-19 pandemic. But the targeted design criteria would focus on compliance with Shneiderman's principles of designing comprehensible, predictable and controllable user interfaces.

User studies would need to evaluate the gesture interface presented in the prototype along those principles. The following tasks could be designed for players to complete, during which think-aloud data and video recordings of the physical and the digital performance (via screen capture) could be collected for qualitative studies:
1. Use gestures to command the companion to fetch an item (no time constraint).
2. As part of the gameplay, a fox has captured the player's valuables. Before the fox run a way, the player needs to use gestures to direct the companion to attach the fox, then fetch the valuables back to the player (with time constraint of 1 minute).

For quantitative study, collect and analyze the result of the following questionnaires for each user: NASA TLX (Task Load Index), CSI (Creativity Support Index), and UES (User Engagement Scale) for VR game with gesture hand interface.